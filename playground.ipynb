{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If one author is likely to write \"?good?\" articles he/she might receive more comments. \n",
    "Calculate entropy of each keyword and try to grep all articles with 20-ish words. \n",
    "Number of images should be one of the features. Also try binary variable - has image (1, 0) \n",
    "One feature should be connected with the hour of the posing. \n",
    "Number of paragraphs should be a feature. \n",
    "Length of the title can be a feature. "
   ],
   "id": "b38f21550463ee9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_json(data_path: str) -> list:\n",
    "    with gzip.open(data_path, 'rt', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define all constants",
   "id": "56ed8530f0a5ecda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MINIMUM_REPEAT = 41",
   "id": "8dbd08f98d3762f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gpt_imputer(paragraphs: list, all_gpt_words: set) -> list:\n",
    "    surrogate_words = list()\n",
    "    for paragraph in paragraphs:\n",
    "        for word in paragraph:\n",
    "            if word in all_gpt_words:\n",
    "                surrogate_words.append(word)\n",
    "                print(word)\n",
    "    return surrogate_words"
   ],
   "id": "a8a916045c11231a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw = read_json('rtvslo_train.json.gzip')\n",
    "print(len(raw))\n",
    "# head = raw[:50]"
   ],
   "id": "c29eeebbd19e356f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "head = raw[:300]",
   "id": "fdb3c6f18607fd61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "no_figures = []\n",
    "for article in head:\n",
    "    # if 'figures' not in article.keys():\n",
    "    #     no_figures.append(0)\n",
    "    #     continue\n",
    "    no_figures.append(len(article['figures']))\n",
    "print(no_figures)"
   ],
   "id": "30c6e53ab035cd2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "times = []\n",
    "for article in head:\n",
    "    timestamp = datetime.fromisoformat(article['date'])\n",
    "    times.append(timestamp.hour) # .weekday() and for weekend < 6 ture 1\n",
    "print(times)"
   ],
   "id": "7ee5ad02edf81069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# all_gpt_keywords = set([article['gpt_keywords'] for article in head if 'gpt_keywords' in article.keys()])\n",
    "all_gpt_keywords = set()\n",
    "for article in head:\n",
    "    if 'gpt_keywords' in article.keys():\n",
    "        all_gpt_keywords.update(article['gpt_keywords'])\n",
    "print(all_gpt_keywords)"
   ],
   "id": "6a5f79af51a33474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "absolute_all_keywords = list()\n",
    "keywords_counter = {}\n",
    "for article in raw:\n",
    "    if 'gpt_keywords' in article.keys():\n",
    "        absolute_all_keywords.extend(article['gpt_keywords'])\n",
    "    else:\n",
    "        absolute_all_keywords.extend(article['keywords'])\n",
    "keywords_counter = Counter(absolute_all_keywords)\n",
    "# print(sorted(keywords_counter.items(), key=lambda x : x[1], reverse=False))\n",
    "df = pd.DataFrame(list(keywords_counter.values()), columns=['values'])\n",
    "print(df.describe())\n",
    "minimum_repeat = df.quantile(0.99).values[0]\n",
    "chosen = sum(1 for value in keywords_counter.values() if value > minimum_repeat)\n",
    "# print(chosen)\n",
    "important_keywords = [key for key, value in keywords_counter.items() if value > minimum_repeat]\n",
    "print(important_keywords)\n"
   ],
   "id": "c5cbc5b64476f8a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_keys = list()\n",
    "for article in head:\n",
    "    unique_keys.extend([*article.keys()])\n",
    "print(sorted(list(set(unique_keys))))"
   ],
   "id": "1e2240cf4a6c38c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for article in head:\n",
    "#     topic_pair = article['url'].split('/')[3:5]\n",
    "#     if 'sport' in topic_pair:\n",
    "#         article['topic'] = f\"{topic_pair[0]}-{topic_pair[1]}\"\n",
    "#     elif 'topic' not in article.keys():\n",
    "#         article['topic'] = f\"{topic_pair[0]}\"\n",
    "    # else:\n",
    "    #     if article['topic'] != topic_pair[0]:\n",
    "    #         print(article['topic'], \"suggested:\", topic_pair[0])"
   ],
   "id": "b1ff45579b658b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.json_normalize(head, meta=['date', 'title', 'author', 'url'])",
   "id": "8341e671bb76082",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_drop = ['authors', 'id', 'lead', 'category']\n",
    "stay = ['url', 'date', 'figures', 'gpt_keywords', 'keywords', 'paragraphs', 'lead', 'title', 'topics', 'n_comments']\n",
    "df = df.drop(columns=to_drop)\n",
    "print(df)\n",
    "print(df.columns)"
   ],
   "id": "f92c19f3f582fdb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_topic(url):\n",
    "    topic_pair = url.split('/')[3:5]\n",
    "    result = \"\"\n",
    "    if 'sport' in topic_pair:\n",
    "        result = f\"{topic_pair[0]}-{topic_pair[1]}\"\n",
    "    else:\n",
    "        result = f\"{topic_pair[0]}\"\n",
    "    return result"
   ],
   "id": "83537c7d30c5e655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def keywords_filter(keywords):\n",
    "    result_list = []\n",
    "    for word in keywords:\n",
    "        if word in important_keywords:\n",
    "            result_list.append(word)\n",
    "    return result_list"
   ],
   "id": "2e52fd099a99abd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_extra_features(X):\n",
    "    X['datetime'] = pd.to_datetime(X['date'])\n",
    "    X['hours'] = X['datetime'].dt.hour\n",
    "    X['weekend'] = X['datetime'].dt.weekday > 4\n",
    "    X['title_length'] = X['title'].apply(len)\n",
    "    df_topic = pd.DataFrame(X['url'])\n",
    "    X['topics'] = df_topic['url'].apply(extract_topic)\n",
    "    X[X['gpt_keywords'] == ''] = X['keywords']\n",
    "    X['gpt_keywords'] = X['gpt_keywords'].apply(keywords_filter)\n",
    "    X['images'] = X['figures'].apply(len)\n",
    "    X['article_length'] = X['paragraphs'].apply(len)\n",
    "    \n",
    "    to_drop_after_processing = ['datetime', 'date', 'title', 'url', 'keywords', 'figures', 'paragraphs']\n",
    "    return X.drop(columns=to_drop_after_processing, inplace=False)\n",
    "    # return X\n",
    "\n",
    "attr_adder = FunctionTransformer(add_extra_features, validate=False)\n",
    "dr_reset = df.reset_index(drop=True)\n",
    "articles = attr_adder.fit_transform(dr_reset, important_keywords)\n",
    "print(articles)\n",
    "print([*articles.columns])"
   ],
   "id": "48327221888457d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop labels for training set\n",
    "y = articles['n_comments'].copy()\n",
    "X = articles.drop('n_comments', axis=1, inplace=False).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Learning set size: {:d}\\nTest set size: {:d}\".format(len(X_train), len(X_test)))\n"
   ],
   "id": "dbba5cbef58d50c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlb = MultiLabelBinarizer(classes=np.array(important_keywords))\n",
    "out = mlb.fit_transform(X_train['gpt_keywords'][:5])\n",
    "print(mlb.get_params())\n",
    "print(out)"
   ],
   "id": "d64b7646d2dc216e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_attributes = ['title_length', 'article_length', 'images']\n",
    "categorical_attributes = ['weekend', 'topics', 'hours']\n",
    "multi_label_attributes = ['gpt_keywords']\n",
    "\n",
    "ct = make_column_transformer()\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), numerical_attributes),\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'), categorical_attributes),\n",
    "    ('multi_label_attributes', MultiLabelBinarizer(), multi_label_attributes)\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('transformer', column_transformer),\n",
    "    ('estimatior', LinearRegression())\n",
    "])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(score)"
   ],
   "id": "2b39e466fe6d4efc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
